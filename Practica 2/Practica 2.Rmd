---
title: "Practica 2"
author: "Carlos Pastor"
date: "2023-11-21"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages}
# install Packages
# install.packages("dplyr")

library (XML) 
library (httr2)
library (rvest)
library (tidyr)
library (dplyr)
```


# Pregunta 1
Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado. El primer paso para realizar tareas de crawling y scraping es poder descargar los datos de la web. #Para esto usaremos la capacidad de R y de sus librerías (httr y XML) para descargar webs y almacenarlas en variables que podamos convertir en un formato fácil de analizar (p.e.de HTML a XML). 
```{r p1}
URL <- 'https://www.mediawiki.org/wiki/MediaWiki'
Website <- GET(URL)
html_content <- content(Website, "text")
Texto <- read_html(html_content)
```


# Pregunta 2
Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”). En las cabeceras web encontramos información como el título, los ficheros de estilo visual, y meta-información como el nombre del autor de la página, una descripción de esta, el tipo de codificación de esta, o palabras clave que indican qué tipo de información contiene la página. Una vez descargada la página, y convertida a un formato analizable (como XML), buscaremos los elementos de tipo “title”. P.e. “<title>Titulo de Página</title>”
```{r p2}
html_title <- Texto %>% html_element("title") %>% html_text2()
print (html_title)
```


# Pregunta 3
Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL. Vamos a extraer, usando las funciones de búsqueda XML, todos los enlaces quesalen de esta página con tal de listarlos y poder descargarlas más tarde. Sabemos que estos son elementos de tipo “<a>”, que tienen el atributo “href” para indicar la URL del enlace. P.e. “<a href = ‘enlace’>Texto del Enlace</a>”. Del enlace nos quedaremos con la URL de destino y con el valor del enlace (texto del enlace)
```{r p3}
Enlaces <- Texto %>% html_elements("a")
url_enlaces <- Enlaces %>% html_attr("href")
texto_enlaces <- Enlaces %>% html_text2()
dd <- data.frame(
  enlaces = url_enlaces,
  texto = texto_enlaces,
  stringsAsFactors = F
)
```


# Pregunta 4
Generar una tabla con cada enlace encontrado, indicando el texto queacompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo. En este paso nos interesa reunir los datos obtenidos en el anterior paso. Tendremos que comprobar, para cada enlace, cuantas veces aparece.
```{r p4}
tabla_enlaces <- dd %>% group_by(enlaces, texto) %>% summarise(contador = n(), .groups()="drop")
print (tabla_enlaces)
```


# Pregunta 5
Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL). En este paso podemos usar la función HEAD de la librería “httr”, que en vez de descargarse la página como haría GET, solo consultamos los atributos de la página o fichero destino. HEAD nos retorna una lista de atributos, y de entre estos hay uno llamado “header” que contiene más atributos sobre la página buscada. Si seguimos podemos encontrar el “status_code” en “resultado$status_code”. El “status_code” nos indica el resultado de la petición de página o fichero. Este código puede indicar que la petición ha sido correcta (200), que no se ha encontrado (404), que el acceso está restringido (403), etc.
• Tened en cuenta que hay enlaces con la URL relativa, con forma “/xxxxxx/xxxxx/a.html”. En este caso, podemos indicarle como “handle” el dominio de la página que estamos tratando, o añadirle el dominio a la URL con la función “paste”. 
• Tened en cuenta que puede haber enlaces externos con la URL absoluta, con forma “http://xxxxxx/xxxx/a.html” (o https), que los trataremos directamente.
• Tened en cuenta que puede haber enlaces que apunten a subdominios distintos, con forma “//subdominio/xxxx/xxxx/a.html”. En este caso podemos adjuntarle el prefijo “https:” delante, convirtiendo la URL en absoluta.
• Tened en cuenta URLS internas con tags, como por ejemplo “#search-p”. Estos apuntan a la misma página en la que estamos, pero diferente altura de página. Equivale a acceder a la URL relativa de la misma página en la que estamos.

Es recomendado poner un tiempo de espera entre petición y petición de pocos segundos (comando “Sys.sleep”), para evitar ser “baneados” por el servidor. Para poder examinar las URLs podemos usar expresiones regulares, funciones como “grep”, o mirar si en los primeros caracteres de la URL encontramos “//” o “http”.  Para tratar las URLs podemos usar la ayuda de la función “paste”, para manipular cadenas de caracteres y poder añadir prefijos a las URLs si fuera necesario

```{r p5-1}
# Comprobar si la URL es relativa y ajustarla según las condiciones

check_link_status <- function(link) {
  Sys.sleep(2)
  print(link)
  response <- httr2::request(link) |> req_error(is_error = \(resp) FALSE) |> req_perform()
  return(response$status_code == 200)
  # if (http_status(response)$status_code == 200) {
  #   return("Activo")
  # } else {
  #   return("No activo")
  # }
}
```

```{r p5-2}
relative2absolute <- function(my_url) {
  # version 1
  base_domain <- "https://www.mediawiki.org/wiki/MediaWiki"
  if (grepl("^(/|#)", my_url, perl = T)) {
    my_url <- paste(base_domain, my_url,sep = "")
  }
  my_url
  
  
  # version 2
  # if (substring(url_enlaces, 1, 2) == "//") { 
  #   url_abs <- paste("https:", url_enlaces, sep = "")
  #   } else if (substring(url_enlaces, 1, 4) == "http") {
  #     url_abs <- url_enlaces
  #     } else if (substring(url_enlaces, 1, 1) == "/") {
  #       url_abs <- paste(url, url_enlaces, sep = "")
  #       } else {
  #         url_abs <- url_enlaces
  #         enlace_estado <- c(enlace_estado, check_link_status(url_abs))
  #         enlaces_df <- dd2(Texto = enlaces, URL = url_enlaces, Estado = estado_enlace)
  #       }
}
tabla_enlaces$enlaces_absolutos <- NULL
tabla_enlaces$es_absoluto <- !grepl("^(/|#)", tabla_enlaces$enlaces, perl = T)
tabla_enlaces$enlace_absoluto <- sapply(tabla_enlaces$enlaces, relative2absolute)
```


```{r comprobar_estado}
tabla_enlaces$activo <- sapply(tabla_enlaces$enlace_absoluto, check_link_status)

```